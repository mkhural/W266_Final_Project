{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W266_GRUEN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Clk89frEhz1S",
        "outputId": "743366c3-1217-4626-c0e2-967e8d1389b5"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/WanzhengZhu/GRUEN/master/requirements.txt "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-31 18:54:19--  https://raw.githubusercontent.com/WanzhengZhu/GRUEN/master/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 80 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]      80  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-31 18:54:19 (3.08 MB/s) - ‘requirements.txt’ saved [80/80]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q-GYp4xVst9",
        "outputId": "6d479d68-aa62-43e7-9c7a-701418d0c284"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/WanzhengZhu/GRUEN/master/Main.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-31 18:54:22--  https://raw.githubusercontent.com/WanzhengZhu/GRUEN/master/Main.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10394 (10K) [text/plain]\n",
            "Saving to: ‘Main.py’\n",
            "\n",
            "\rMain.py               0%[                    ]       0  --.-KB/s               \rMain.py             100%[===================>]  10.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-31 18:54:22 (50.5 MB/s) - ‘Main.py’ saved [10394/10394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-mKM2G1_JCy"
      },
      "source": [
        "### Make sure './cola_model/bert-base-cased' files uploaded\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ8qoXMj3DVx",
        "outputId": "dc1e8a77-3197-4f2e-ea13-a0f1562a60e4"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.9.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.41.1)\n",
            "Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (3.4.0)\n",
            "Collecting wmd==1.3.1\n",
            "  Downloading wmd-1.3.1.tar.gz (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 5.0 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement difflib (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for difflib\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6FPYOhgXBHl",
        "outputId": "1ca72d73-64f1-44e7-b4b3-068ac29e6105"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051301 sha256=cc7eb828dff8e641694e83718dc891e6aeef64b78b144c2bdc0deb2669ad0a40\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bevrjxxf/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cgbSoMPWena",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "67e6456a-dde8-4330-ae1b-08ae905b06d6"
      },
      "source": [
        "!pip install wmd==1.3.1\n",
        "import Main"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wmd==1.3.1\n",
            "  Using cached wmd-1.3.1.tar.gz (104 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from wmd==1.3.1) (1.19.5)\n",
            "Building wheels for collected packages: wmd\n",
            "  Building wheel for wmd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wmd: filename=wmd-1.3.1-cp37-cp37m-linux_x86_64.whl size=630270 sha256=1fab7f80bc6823906db3a2aac87accf2321d30095634cc3427dce536fee4bbb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/b5/b5/03c4bf11fd9f19857eacf41f8308e350ee7c5f08321ce6be51\n",
            "Successfully built wmd\n",
            "Installing collected packages: wmd\n",
            "  Attempting uninstall: wmd\n",
            "    Found existing installation: wmd 1.3.2\n",
            "    Uninstalling wmd-1.3.2:\n",
            "      Successfully uninstalled wmd-1.3.2\n",
            "Successfully installed wmd-1.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "libwmdrelax",
                  "wmd"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NuXFvyOZIU3"
      },
      "source": [
        "import difflib\n",
        "import editdistance\n",
        "import math\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "import string\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from tqdm import tqdm\n",
        "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, BertForMaskedLM\n",
        "from transformers import glue_convert_examples_to_features\n",
        "from transformers.data.processors.utils import InputExample\n",
        "from wmd import WMD"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfbcW_gfZJua"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4xAUp08ZOBo"
      },
      "source": [
        "\"\"\" Processing \"\"\"\n",
        "def preprocess_candidates(candidates):\n",
        "    for i in range(len(candidates)):\n",
        "        candidates[i] = candidates[i].strip()\n",
        "        candidates[i] = '. '.join(candidates[i].split('\\n\\n'))\n",
        "        candidates[i] = '. '.join(candidates[i].split('\\n'))\n",
        "        candidates[i] = '.'.join(candidates[i].split('..'))\n",
        "        candidates[i] = '. '.join(candidates[i].split('.'))\n",
        "        candidates[i] = '. '.join(candidates[i].split('. . '))\n",
        "        candidates[i] = '. '.join(candidates[i].split('.  . '))\n",
        "        while len(candidates[i].split('  ')) > 1:\n",
        "            candidates[i] = ' '.join(candidates[i].split('  '))\n",
        "        myre = re.search(r'(\\d+)\\. (\\d+)', candidates[i])\n",
        "        while myre:\n",
        "            candidates[i] = 'UNK'.join(candidates[i].split(myre.group()))\n",
        "            myre = re.search(r'(\\d+)\\. (\\d+)', candidates[i])\n",
        "        candidates[i] = candidates[i].strip()\n",
        "    processed_candidates = []\n",
        "    for candidate_i in candidates:\n",
        "        sentences = sent_tokenize(candidate_i)\n",
        "        out_i = []\n",
        "        for sentence_i in sentences:\n",
        "            if len(sentence_i.translate(str.maketrans('', '', string.punctuation)).split()) > 1:  # More than one word.\n",
        "                out_i.append(sentence_i)\n",
        "        processed_candidates.append(out_i)\n",
        "    return processed_candidates\n",
        "\n",
        "\n",
        "\"\"\" Scores Calculation \"\"\"\n",
        "def get_lm_score(sentences):\n",
        "    def score_sentence(sentence, tokenizer, model):\n",
        "        # if len(sentence.strip().split()) <= 1:\n",
        "        #     return 10000\n",
        "        tokenize_input = tokenizer.tokenize(sentence)\n",
        "        if len(tokenize_input) > 510:\n",
        "            tokenize_input = tokenize_input[:510]\n",
        "        input_ids = torch.tensor(tokenizer.encode(tokenize_input)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            loss = model(input_ids, labels=input_ids)[0]\n",
        "        return math.exp(loss.item())\n",
        "\n",
        "    model_name = 'bert-base-cased'\n",
        "    model = BertForMaskedLM.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    lm_score = []\n",
        "    for sentence in tqdm(sentences):\n",
        "        if len(sentence) == 0:\n",
        "            lm_score.append(0.0)\n",
        "            continue\n",
        "        score_i = 0.0\n",
        "        for x in sentence:\n",
        "            score_i += score_sentence(x, tokenizer, model)\n",
        "        score_i /= len(sentence)\n",
        "        lm_score.append(score_i)\n",
        "    return lm_score\n",
        "\n",
        "\n",
        "def get_cola_score(sentences):\n",
        "    def load_pretrained_cola_model(model_name, saved_pretrained_CoLA_model_dir):\n",
        "        config_class, model_class, tokenizer_class = (BertConfig, BertForSequenceClassification, BertTokenizer)\n",
        "        config = config_class.from_pretrained(saved_pretrained_CoLA_model_dir, num_labels=2, finetuning_task='CoLA')\n",
        "        tokenizer = tokenizer_class.from_pretrained(saved_pretrained_CoLA_model_dir, do_lower_case=0)\n",
        "        model = model_class.from_pretrained(saved_pretrained_CoLA_model_dir, from_tf=bool('.ckpt' in model_name), config=config).to(device)\n",
        "        model.eval()\n",
        "        return tokenizer, model\n",
        "\n",
        "    def evaluate_cola(model, candidates, tokenizer, model_name):\n",
        "\n",
        "        def load_and_cache_examples(candidates, tokenizer):\n",
        "            max_length = 128\n",
        "            examples = [InputExample(guid=str(i), text_a=x) for i,x in enumerate(candidates)]\n",
        "            features = glue_convert_examples_to_features(examples, tokenizer, label_list=[\"0\", \"1\"], max_length=max_length, output_mode=\"classification\")\n",
        "            # Convert to Tensors and build dataset\n",
        "            all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "            all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "            all_labels = torch.tensor([0 for f in features], dtype=torch.long)\n",
        "            all_token_type_ids = torch.tensor([[0.0]*max_length for f in features], dtype=torch.long)\n",
        "            dataset = torch.utils.data.TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "            return dataset\n",
        "\n",
        "        eval_dataset = load_and_cache_examples(candidates, tokenizer)\n",
        "        eval_dataloader = torch.utils.data.DataLoader(eval_dataset, sampler=torch.utils.data.SequentialSampler(eval_dataset), batch_size=max(1, torch.cuda.device_count()))\n",
        "        preds = None\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            model.eval()\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[3]}\n",
        "                if model_name.split('-')[0] != 'distilbert':\n",
        "                    inputs['token_type_ids'] = batch[2] if model_name.split('-')[0] in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
        "                outputs = model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "        return preds[:, 1].tolist()\n",
        "\n",
        "    def convert_sentence_score_to_paragraph_score(sentence_score, sent_length):\n",
        "        paragraph_score = []\n",
        "        pointer = 0\n",
        "        for i in sent_length:\n",
        "            if i == 0:\n",
        "                paragraph_score.append(0.0)\n",
        "                continue\n",
        "            temp_a = sentence_score[pointer:pointer + i]\n",
        "            paragraph_score.append(sum(temp_a) / len(temp_a))\n",
        "            pointer += i\n",
        "        return paragraph_score\n",
        "\n",
        "    model_name = 'bert-base-cased'\n",
        "    saved_pretrained_CoLA_model_dir = './cola_model/' + model_name + '/'\n",
        "    tokenizer, model = load_pretrained_cola_model(model_name, saved_pretrained_CoLA_model_dir)\n",
        "    candidates = [y for x in sentences for y in x]\n",
        "    sent_length = [len(x) for x in sentences]\n",
        "    cola_score = evaluate_cola(model, candidates, tokenizer, model_name)\n",
        "    cola_score = convert_sentence_score_to_paragraph_score(cola_score, sent_length)\n",
        "    return cola_score\n",
        "\n",
        "\n",
        "def get_grammaticality_score(processed_candidates):\n",
        "    lm_score = get_lm_score(processed_candidates)\n",
        "    cola_score = get_cola_score(processed_candidates)\n",
        "    grammaticality_score = [1.0 * math.exp(-0.5*x) + 1.0 * y for x, y in zip(lm_score, cola_score)]\n",
        "    grammaticality_score = [max(0, x / 8.0 + 0.5) for x in grammaticality_score]  # re-scale\n",
        "    return grammaticality_score\n",
        "\n",
        "\n",
        "def get_redundancy_score(all_summary):\n",
        "    def if_two_sentence_redundant(a, b):\n",
        "        \"\"\" Determine whether there is redundancy between two sentences. \"\"\"\n",
        "        if a == b:\n",
        "            return 4\n",
        "        if (a in b) or (b in a):\n",
        "            return 4\n",
        "        flag_num = 0\n",
        "        a_split = a.split()\n",
        "        b_split = b.split()\n",
        "        if max(len(a_split), len(b_split)) >= 5:\n",
        "            longest_common_substring = difflib.SequenceMatcher(None, a, b).find_longest_match(0, len(a), 0, len(b))\n",
        "            LCS_string_length = longest_common_substring.size\n",
        "            if LCS_string_length > 0.8 * min(len(a), len(b)):\n",
        "                flag_num += 1\n",
        "            LCS_word_length = len(a[longest_common_substring[0]: (longest_common_substring[0]+LCS_string_length)].strip().split())\n",
        "            if LCS_word_length > 0.8 * min(len(a_split), len(b_split)):\n",
        "                flag_num += 1\n",
        "            edit_distance = editdistance.eval(a, b)\n",
        "            if edit_distance < 0.6 * max(len(a), len(b)):  # Number of modifications from the longer sentence is too small.\n",
        "                flag_num += 1\n",
        "            number_of_common_word = len([x for x in a_split if x in b_split])\n",
        "            if number_of_common_word > 0.8 * min(len(a_split), len(b_split)):\n",
        "                flag_num += 1\n",
        "        return flag_num\n",
        "\n",
        "    redundancy_score = [0.0 for x in range(len(all_summary))]\n",
        "    for i in range(len(all_summary)):\n",
        "        flag = 0\n",
        "        summary = all_summary[i]\n",
        "        if len(summary) == 1:\n",
        "            continue\n",
        "        for j in range(len(summary) - 1):  # for pairwise redundancy\n",
        "            for k in range(j + 1, len(summary)):\n",
        "                flag += if_two_sentence_redundant(summary[j].strip(), summary[k].strip())\n",
        "        redundancy_score[i] += -0.1 * flag\n",
        "    return redundancy_score\n",
        "\n",
        "\n",
        "def get_focus_score(all_summary):\n",
        "    def compute_sentence_similarity():\n",
        "        nlp = spacy.load('en_core_web_md')\n",
        "        nlp.add_pipe(WMD.SpacySimilarityHook(nlp), last=True)\n",
        "        all_score = []\n",
        "        for i in range(len(all_summary)):\n",
        "            if len(all_summary[i]) == 1:\n",
        "                all_score.append([1.0])\n",
        "                continue\n",
        "            score = []\n",
        "            for j in range(1, len(all_summary[i])):\n",
        "                doc1 = nlp(all_summary[i][j-1])\n",
        "                doc2 = nlp(all_summary[i][j])\n",
        "                try:\n",
        "                    score.append(1.0/(1.0 + math.exp(-doc1.similarity(doc2)+7)))\n",
        "                except:\n",
        "                    score.append(1.0)\n",
        "            all_score.append(score)\n",
        "        return all_score\n",
        "\n",
        "    all_score = compute_sentence_similarity()\n",
        "    focus_score = [0.0 for x in range(len(all_summary))]\n",
        "    for i in range(len(all_score)):\n",
        "        if len(all_score[i]) == 0:\n",
        "            continue\n",
        "        if min(all_score[i]) < 0.05:\n",
        "            focus_score[i] -= 0.1\n",
        "    return focus_score\n",
        "\n",
        "\n",
        "def get_gruen(candidates):\n",
        "    processed_candidates = preprocess_candidates(candidates)\n",
        "    grammaticality_score = get_grammaticality_score(processed_candidates)\n",
        "    redundancy_score = get_redundancy_score(processed_candidates)\n",
        "    focus_score = get_focus_score(processed_candidates)\n",
        "    gruen_score = [min(1, max(0, sum(i))) for i in zip(grammaticality_score, redundancy_score, focus_score)]\n",
        "    return gruen_score"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVOcAmM5ZlPh",
        "outputId": "a87cb26a-91f8-4b00-d44b-cbfe8294c69c"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGzNQRPzl1Hh"
      },
      "source": [
        "# If pytoch model doesn't run:\n",
        "# !pip install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "hSx-wrOgZU9D",
        "outputId": "c97f3cfc-c847-413b-aa9f-f1a22ff008d8"
      },
      "source": [
        "candidates = [\"This is a good example.\",\n",
        "                  \"This is a bad example. It is ungrammatical and redundant. Orellana shown red card for throwing grass at Sergio Busquets. Orellana shown red card for throwing grass at Sergio Busquets.\"]\n",
        "gruen_score = Main.get_gruen(candidates)\n",
        "print(gruen_score)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.27it/s]\n",
            "Evaluating: 100%|██████████| 5/5 [00:02<00:00,  2.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-413d76a1fe8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m candidates = [\"This is a good example.\",\n\u001b[1;32m      2\u001b[0m                   \"This is a bad example. It is ungrammatical and redundant. Orellana shown red card for throwing grass at Sergio Busquets. Orellana shown red card for throwing grass at Sergio Busquets.\"]\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgruen_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gruen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgruen_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Main.py\u001b[0m in \u001b[0;36mget_gruen\u001b[0;34m(candidates)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mgrammaticality_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_grammaticality_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mredundancy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_redundancy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mfocus_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_focus_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mgruen_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammaticality_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredundancy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocus_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgruen_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Main.py\u001b[0m in \u001b[0;36mget_focus_score\u001b[0;34m(all_summary)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mall_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_sentence_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0mfocus_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Main.py\u001b[0m in \u001b[0;36mcompute_sentence_similarity\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_focus_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_sentence_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWMD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpacySimilarityHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mall_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4610czsZZAZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}